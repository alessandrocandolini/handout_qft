%*******************************************************
% Prefazione 
%*******************************************************


\myChapter*{Aphorisms}
\markboth{\spacedlowsmallcaps{Aphorisms}}{\spacedlowsmallcaps{Aphorisms}} 
%\mtcaddchapter[\numberline{}\tocEntry{Aphorisms}]

		   \addtocontents{toc}{\protect\vspace{\beforebibskip}}
\addcontentsline{toc}{chapter}{\numberline{}\tocEntry{Aphorisms}}


The \emph{lietemotiv} of these notes is \emph{renormalization}. Let's sketch a
summary to grab the key
ideas. The section is mainly inspired by the introductory chapter of
Zinn-Justin.

QFT in its most direct formulations run into a severe conceptual
difficulty.
%with a \emph{severe conceptual
%   drawback}, namely the appearance of infinities in the
%calculation  of physical quantities. 
QFT crashes due to the occurrence of infinities in the calculations of physical
quantities.

%That's awful. QFT seems to have no predictive power. 
To overcome this difficulty and avoid the occurrence of infinities, an empirical, somewhat \emph{ad hoc} (but
systematic) procedure,
which goes under the name of renormalization, was eventually developed which allows extracting from
(meaningless) divergent mathematical expressions
(meaningfull) finite numerical predictions to be compared with experiments.


Often in mathematics one meets expressions which naively speaking are divergent
but you can append a meaning to them eg via analytic continuation or by
regularize them in a suitable way.
But here the story is different:
experimental data are needed in order the renormalization algorithm to work.
You have to provide \emph{input} experimental parameters in order the
renomalization machinary to work. 
Renormalization heals the divergences of the theory by sweep them 
under the carpet of few external parameters.
In other words, infinities are absorbed into few experimental parameters.

The renormalization recipe works in this spirit: you have to carefully
distinguish between \emph{bare} quantities (\eg, mass, electric charge) and
\emph{effective}
quantities. The latter ones are those which actually have to be related to
experiments, while the former ones are additional auxiliary parameters of the
physical model, which are necessary in order the renormalization recipe to work
but whose physical meaning remains somewhat obscured.
At this level, it might be said that renornalization is a way of organizing the
calculations in order to get finite results from expressions which naively
speaking would otherwise led to infinities.

From a pragmatic point of view, 
renormalization works: It 
has allowed and still allows calculations of increasing precision.
In fact, the renormalization procedure would hardly have been convincing if the
predictions were not confirmed with increasing precision by experiments.

Here comes another major point. 
Only later, did the procedure find a satisfactory physical interpretation which
clarify the deep origin of renormalization and enlight the role of
renormalizable QFTs.
   The problem of infinities is related to an unexpected phenomenon:
   \emph{Renormalization is related to the non-decoupling of very different
      lenght scales}
Today, Those who are familiar with Kenneth Wilson's ideas and the renormalization
group, will immediately say that actually there is no divergence.
More or less, the story goes like this: 
Every QFT requires an ultraviole completion (thus being an effective theory).
The only difference between renormalizable and non-renormalizable QFTs lies in
the fact that the former are \emph{insensitive} to the ultraviolet data (which
can be absorbed in a few low-energy parameters) while the latter depend on the
details of the ultraviole completion.




 
